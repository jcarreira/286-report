\begin{figure}[t!]
  \begin{center}
    \includegraphics[scale=0.40]{images_graphs/waterfall/Rplots_illustrator.pdf}
  \end{center}
  \caption{}
  \label{fig:SparkStreaming_time_breakdown}
\end{figure}

To better understand the performance and limitations of Spark Streaming we conducted a benchmark study of the system.

To this end we have instrumented Spark Streaming.
Our instrumentation code allows us to track a subset of the stream data as it flows through the system.
We record at which moments of time some of the records pass through some of the components.

To exercise Spark Streaming we have used a synthetic workload.
%To this end we used a synthetic workload to exercise Spark Streaming and measure how the system behaves under different scenarios.
This workload consists of an application that listens for a stream of text records.
Each record has size between 15-25 bytes and holds a unique ID and a timestamp of time when the data was generated.
For each micro-batch, the application computes and records the end-to-end latency of the first 10 records in the micro-batch.

The computation performed by this workload is very lightweight.
We believe that this is in line with the type of computations used with Spark Streaming.
First, typical streaming tasks consist of filtering and/or simple aggregations.
These are operations that scale linearly with the amount of data in each batch.
Secondly, there is extensive work on optimizing the performance of query processing, both in batch and streaming contexts.
Finally, many steps of the computations performed in Spark and Spark Streaming are embarrassingly parallel. 
This means that developers can make use of more hardware to get better performance. 

%This is because first, the computation time is almost always orthogonal to the architecture used.
%Secondly, there is other work aimed at improving computation times in streaming or MapReduce systems.
%    There is parallel work on improving the performance of streaming computations that can be used to improve the execution times of tasks.

To run this benchmark, we have deployed a Receiver and a record generator in two different machines in the same cluster.

The record generator generates as many records as necessary to achieve a user-specified throughput.
Each record contains a timestamp that corresponds to the moment of creation of the record as well as a unique ID.
The Receiver uses the Spark Streaming API to consume this data. 
A task is generated periodically, according to a user-defined frequency, and scheduled to a node where the data is processed.

\begin{figure}[t!]
  \begin{center}
    \includegraphics[scale=0.35]{images_graphs/batchsize_vs_latency/batchsize_vs_latency_illustrator.pdf}
  \end{center}
  \caption{\joao{Maybe put the legend inside the graph so the graph can be larger?}}
  \label{fig:Batchsize_vs_latency}
\end{figure}

We have instrumented the code of Spark Streaming to record the time at which the record transitions a phase (e.g., a task to process a record is scheduled).
%#understand how much time each record stays on each phase of the streaming process.
This allows us to gather 1) the time each record spends on each phase of the streaming process, and 2) the time between the generation of each record and the moment of computation -- end-to-end latency.

%When processing records, the time between the generation of each record and the moment of computation is captured. 
%We refer to this value throughout the paper as the end-to-end latency. 

The results of the first experiment we conducted are captured in Figure~\ref{fig:Batchsize_vs_latency}.
In this graph we show the average end-to-end latency obtained when running Spark Streaming with different batch windows and throughputs. 
Changing the batch window configuration allows us to tune the responsiveness of the system;
a lower value means that each records spends less time in memory waiting for a task to be spawned in order to process that same record. 
Varying the number of records generated by the stream source (throughput), allows us to understand how 
Spark behaves when it has to do more/less work per unit of time and how that affects latency.

As expected, as we instruct Spark Streaming to spawn tasks more frequently -- smaller batch window -- the average end-to-end latency time decreases.
However, at some point decreasing the batch window has a pernicious effect on the resulting latency.
Moreover, as we increase the throughput the end-to-end latency increases.
We have found that the Spark Streaming's Receiver can be a source of slowdown.
For instance, it is not able to receive more than roughly 30 MB/s (1.5M records).
This has to do with the fact that this component synchronously receives and stores streaming data. 
