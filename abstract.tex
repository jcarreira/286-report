The value of stream processing systems stems largely from the timeliness of the results these systems provide.
Early stream processors followed the record-at-a-time approach, servicing each data point as soon as it arrives at the system. While these systems provide good latency, their behaviors become less desirable when applications require high throughput, fault tolerance, or usage of stateful or blocking operators. More recently, systems are developed to follow the micro-batch approach, where many records are processed together as small batches, providing the missing features seemingly at the cost of latency.

Given the situation, we look into a micro-batch streaming system called Spark Streaming, and investigate how well the micro-batch architecture can handle latency-sensitive workloads. 
We instrumented Spark Streaming to understand where time goes and where development effort should be focused on.
%Nowadays, different streaming systems provide different end-to-end time guarantees, and these guarantees are to a large extent a result of each systems architecture. 
%Spark Streaming is built on top of a general batch-processing framework to compute over mini-batches of streaming data. 
%Storm allows the construction of topologies on which data flows to be processed. 
%Spark Streaming is known to provide latencies on the order of seconds while Storm on the order of a few milliseconds. 
%In this paper we investigate the approach taken by Spark Streaming and show that despite its more general architecture (\joao{general architecture may not be the right term}) it can provide latencies below 100ms. 
In this paper we make three contributions. 
First, we provide an analysis of the performance of Spark Streaming, showing the average time breakdown within the system. 
Second, we identify the performance and scalability bottlenecks of Spark Streaming and pinpoint the underlying deficiencies of the system. 
Last, we propose and evaluate several optimizations to reduce the system overhead and achieve lower latency while maintaining throughput.
        
%        To do so, we tackled two problems. The first has to do with overheads related to launching tasks and communicating between nodes. The second has to do with the poor scalability performance when the Spark is subject to a high number of tasks being scheduled. We show that with these two contributions Spark Streaming can support low-latency tasks and scale to demanding low-latency stream workloads.

%Large-scale data processing systems such as Spark and Hadoop MapReduce were developed for clusters built on commodity hardware. In this paper, we argue that current market trends invalidate some of the assumptions made by those systems, and we should therefore revisit the designs and architectures derived from them. For example, newer technologies including InfiniBand offer faster network connectivity, and advances in SSDs continue to drive down the cost of fast non-volatile memory. One new architecture that explores these new technologies is Firebox, a hardware building block for future Warehouse-Scale Computers (WSCs). We use Firebox in our studies, and evaluate the behavior changes of large-scale systems under the new settings. In particular, we measure the performance of Spark and applications in the Spark ecosystem under specific workloads, and critique the validity of its underlying assumptions and design decisions.
