Data analytics applications such as intrusion detection, web search and monitoring, require the computation of results in a timely fashion in order to provide interactivity and responsiveness. 
As enterprise workflows grow increasingly complex and applications become increasingly dependent on other applications, the latency guarantees of their processing systems become a major concern.
For instance, many systems today are designed to meet specific performance metrics or SLOs~\cite{Jockey}.
Applications that do not yield good performance may delay the execution of other systems and lead to loss of revenue and/or bad user experience.

Many data analytics 
systems~\cite{Millwheel,Babu:2001:CQO:603867.603884,TelegraphCQ,Storm,SparkStreaming,Trill,Naiad,Niagara,StreamInsight,Carney:2002:MSN:1287369.1287389,Sullivan:1998:TSM:1268256.1268258,condie2010mapreduce,Brito:2011:SLD:2114498.2116192, ELF} 
have been developed to provide easy-to-use and practical frameworks for stream processing, and they tend to follow one of the two popular approaches to act on inputs.
Systems like Storm or TelegraphCQ handle streams of data by creating pipelines for record-at-a-time processing. In this environment, data flows through the system (potentially through different machines) and data is continuously processed and augmented.
Other systems like Spark Streaming rely on micro-batches. In these systems, records are coalesced into small groups before being processed together as batch jobs.

It is widely believed that to achieve low latency, applications should use the record-at-a-time approach, while for high throughput, the micro-batch model is more appropriate. 
The reason is that even though record-at-a-time systems can start processing data as soon as it is received, processing more records at a time will amortize the overheads associated with processing.
However, because records are not processed right away, the latency per record is higher on average.
In this paper, we are generally concerned with end-to-end latency, which the time between the application sending to data and the application receiving the output.

%However, these two approaches provide different trade-offs (as shown in table X).
The trade-offs between latency and throughput become less simple as applications are scaled out to run in a distributed fashion. In a distributed environment, the record-at-a-time approach has many disadvantages when compared with the micro-batch approach.
First, record-at-a-time systems are not suitable for stateful or blocking operators, as these operators by nature will unboundedly increase memory usage or stall the system. Although techniques such as punctuations~\cite{tucker2003exploiting} can be used, they place extra burden on the application. For micro-batch systems, because batches are of finite size, behaviors of stateful and blocking operators are much easier to define.
Second, record-at-a-time systems require replication or upstream backup techniques to tolerate failures. Neither of these solutions are desirable, because the former requires the usage of fail-over hardware, and the latter usually leads to high recovery times. When records are batched, fault tolerance can be implemented with methods such as backing up to a database~\cite{Storm} or keeping track of lineage information of batches~\cite{SparkStreaming}.
Finally, the programming model for micro-batch systems is very similar to that of a traditional batch system, making the process of developing streaming applications more familiar to developers.

Because of the advantages in the micro-batch model in distributed settings, we want to explore the possibilities of using this model to provide better end-to-end latencies. In this paper, we perform an in-depth analysis of Spark Streaming, a micro-batch streaming engine, and evaluate the system's ability to provide low-latency and high throughput stream processing.
In particular, we intend to answer the question: ``Are the performance limitations of Spark Streaming a consequence of its architecture, or the result of engineering decisions?"
In answering this question, we found two sources of bottlenecks \team{UPDATE} in the Spark Streaming architecture: receiving data and running tasks. We implement enhancements for running tasks, and offer suggestions on how to tune the process of receiving data.
%We show that by addressing these problems, Spark Streaming can provide O(millisecond) latencies and O(100K records/sec) throughput per node for realistic workloads.

While the focus of our analysis is in Spark Streaming, we believe that our analysis and conclusions can inform the design and development of other streaming systems using the micro-batch architecture. The reason we chose Spark Streaming is that there is a fast-growing ecosystem of frameworks such as GraphX, MLlib, and SparkSQL being developed for Spark, the engine on top of which Spark Streaming is built. As a result, our work can potentially have large impact, since developers will be able to take advantage of both the rich tools as well as low end-to-end latency without sacrificing throughput.

The paper is organized as follows: 
in Section 2 we provide an overview of the architecture and workflow of Spark Streaming. 
In Section 3 we motivate this work with a performance study of Spark Streaming across two main dimensions: end-to-end latency and throughput.
In Section 4 we present optimizations aimed at solving some of the architectural deficiencies identified in the previous section.
In Section 5 we present some of the lessons gathered during this work and discuss some of the architectural changes we believe are required to make Spark Streaming provide lower latencies.
In Section 6 we describe how some other contributions relate and complement our work.
Finally, in section 7 we conclude.

