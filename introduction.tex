\noindent Some data analytics applications, like intrusion detection, web search and sensor actuation, require the computation of results in a timely fashion in order to provide interactivity and timely reaction to events.
Several data analytics systems~\cite{Babu:2001:CQO:603867.603884,TelegraphCQ,Storm,SparkStreaming,Trill,Naiad,Niagara,StreamInsight,Carney:2002:MSN:1287369.1287389,Sullivan:1998:TSM:1268256.1268258,Condie:2010:MO:1855711.185573,Brito:2011:SLD:2114498.2116192} provide easy-to-use and deploy frameworks for these types of applications. 
Systems like Storm or XX \team{fill} handle streams of data by creating pipelines for record-at-a-time processing with at-least-once semantics. 
Others, like Spark Streaming offer processing capabilities with micro-batches, where records are coalesced into small groups before being consumed together.

It is widely believed tha for best latency, applications should use the record-at-a-time approach, while for the highest throughput, the micro-batch model is more appropriate. However, distributed streaming applications are not so simple. For example, record-at-a-time systems do not handle stateful or blocking operators very well natively. As another example, components can fail, and any well-designed system should be able to tolerate failures and recover from them. In these areas, the micro-batch approach has a clear advantage, as many traditional techniques used in databases can be applied to it.

% Unless it is explicitly stated, throughout the paper we refer to tasks latency as the time it takes from the time the data from a stream is received to the time the result of processing that data is returned to the user.

Because of the advantages in the micro-batch model, we want to explore the possibilies of using the model to also support low latency. In this paper we perform an in-depth analysis of Spark Streaming, a micro-batch streaming engine, and evaluate the system's ability to provide low-latency and high throughput stream processing.
In particular, we intend to answer the question: ``Are the limitations of Spark Streaming's performance a fundamental result of its architecture, or the result of design decisions?"
In answering the question, we found three sources of bottlenecks in the Spark Streaming architecture: receiving data, scheduling tasks, and running tasks. We implement enhancements for running tasks, and offer suggestion on how to tune the receiving and scheduling. We show that by addressing these problems, Spark Streaming can provide O(millisecond) latencies and O(100K records/sec) throughput per node for realistic workloads.

Even though the focus of our analysis is in Spark Streaming, we believe that our analysis and conclusions can inform the design and development of other streaming systems using the micro-batch architecture.

The paper is organized as follows: 
in Section 2 we provide an overview of the architecture and workflow of Spark Streaming. 
In Section 3 we present a performance study of Spark Streaming across two main dimensions: end-to-end latency and throughput.
In Section 4 we present some optimizations aimed at solving some of the architectural deficiencies identified in the previous section.
In Section 5 we distill our work into a set of lessons about Spark Streaming and low-latency streaming systems.
Finally, in section 6 we conclude.
