\noindent

Some data analytics applications, like intrusion detection, web search and sensor actuation,
require the computation of results in a timely fashion in order to provide interactivity and react to events in a timely fashion.
Several data analytics systems~\cite{Babu:2001:CQO:603867.603884,TelegraphCQ,Storm,SparkStreaming,Trill,Naiad,Niagara,StreamInsight,Carney:2002:MSN:1287369.1287389,Sullivan:1998:TSM:1268256.1268258,Condie:2010:MO:1855711.185573,Brito:2011:SLD:2114498.2116192} provide easy-to-use and deploy frameworks for these types of applications. 
Systems like Storm or XX \team{fill} handle streams of data by creating pipelines for record-at-a-time processing with at-least-once semantics. 
Others, like Spark Streaming offer processing capabilities with micro-batches, where records are coalesced into small groups before being consumed together.

In this paper we show that it is possible to support low-latency and high throughput streaming applications with a general-purpose framework like Spark.
Even though the focus of our analysis is in Spark Streaming and Spark, we believe that our analysis and conclusions can inform the design and development of other Streaming systems.

% Unless it is explicitly stated, throughout the paper we refer to tasks latency as the time it takes from the time the data from a stream is received to the time the result of processing that data is returned to the user.

% TODO:
% Why are we focusing on Spark Streaming?
%
%
Spark Streaming, the focus of our work, is a stream processing engine built on top of Spark~\cite{Spark}, a general-purpose engine for large-scale data processing. It implements the discretized stream (D-Stream) abstraction, and uses a micro-batch approach to process data as it is received. Data is captured by specific applications, called receivers. Receivers are responsible for connecting to the source of data, setting up a Spark Streaming execution and specifying how often to process received records. Spark Streaming creates a batch job (according to the chosen frequency) after a specific amount of time (the batch interval), and processes the data received by all receivers during that period. Every batch job is divided into map and reduce stages, each of which contains tasks that need to be scheduled.
At the heart of the micro-batches approach taken by Spark is a tradeoff between the amount of records Spark can process per unit of time and the time the system takes to return to the user the result of processing a stream record. On one hand, waiting for more records to generate bigger batches allows Spark Streaming to amortize its overheads. On the other hand, the more time the system waits for data, the less responsive the system is.

% Dont need to explain what is a block interval. Too low level for an introduction
%Internally, a variable called block interval controls the period of time before a receiver generates a new block. The total number of blocks generated in a batch interval corresponds to the number of tasks spawned for every stage of the resulting batch job.

In this paper we perform an in-depth analysis of Spark Streaming, and evaluate the system's ability to provide low-latency and high throughput stream processing.
In particular we intend to answer the question: "Are the limitations of Spark Streaming's performance a fundamental result of its architecture, or is the current poor low-latency streaming performance
the result of poor engineering"? (Rephrase, I am being harsh).
We show that by addressing some of Spark Streaming architectural deficiencies Spark Streaming can provide O(millisecond) latencies and O(100K/sec) thoughput for realistic workloads. 
To achieve this

   and introduce a number of improvements to the system so that it can handle applications with latency requirements on the order of hundreds or even tens of milliseconds while sacrificing as little as possible on throughput. More specifically, we focus on three main areas of improvements. The first area is to reduce the overhead of running tasks. As tasks are executed, there are a number of inefficiencies in terms of sending duplicate data and performing redundant computations. The second area is the scheduling scalability. Currently, Spark uses a centralized scheduler, which becomes a bottleneck as the number of tasks needed to be scheduled increases with the increase in number of receivers or decrease in the block interval. The third area of focus is the network overhead. The amount of network communication is proportional to the number of tasks running concurrently. We propose an enhancement that uses newer technology such as InfiniBand to reduce the transmission delay, or remote direct memory access (RDMA) to relieve the burden of sending and receiving network packets from the driver CPU.

The paper is organized as follows: in Section 2 we evaluate the performance of Spark Streaming and motivate the importance of tackling the problems of 1) high task overhead, 2) limited scheduling scalability, and 3) high network communication delay. In Section 3 we discuss the implementation details on solving these problems. In Section 4 we conclude on our findings.


\team{We should have a short background section explaining how Spark works. Things like receivers/executors/workers/driver could be introduced here. 1 clean diagram and everything afterwards with be much easier to understand}
