Having conducted various benchmarks of Spark Streaming for this research, we have found various parameters that can be used to tune the performance of the system. These parameters include the number of Receivers, the batch window size, the size of input records, and whether the input data is serialized. In this section we will describe in detail about their impacts on the end-to-end latency or throughput. We believe these parameters reflect configuration parameters common to micro-batch streaming systems and thus these lessons are generally applicable.

\paragraph{Number of Receivers}
In our synthetic benchmark using 20-byte records, a single Receiver is able to take in at most around 30MB/s, or 1.5M records/s, even after we modified the code to immediate drop instead of storing the data. While the throughput does not seem very high, 1.5M records is 2.5 times the 600K records per second per node recorded by the original Spark Streaming paper. This insight tells us that when evaluating throughput of a system, both the volume of data as well as the size of input data should be taken into consideration.

To overcome the bottleneck in data intake, the solution can be as simple as increasing the number of receivers. It is also possible to obtain the performance gain by setting the number of Receivers larger than the number of physical machines.

\paragraph{Size and Number of Input Records}
As explained by the previous subsection, the size and number of input records affect the system's performance in terms of throughput. As the size of the records increases, the throughput of the system in volume will also increase linearly to a point, since more data is processed per record. Conversely, as the size of input decreases, the Receiver eventually reaches a bottleneck in the number of records that it can handle (around 1.5M/s).

\paragraph{Size of Batch Interval}
While the micro-batch approach increases throughput at the expense of latency by coalescing input data before processing, the size of the batch interval does not form a linear relationship with the throughput. In fact, the best latency is obtained when the batch interval is set to slightly larger than the time it takes to computationally process a batch. This difference accounts for overheads in task spawning, scheduling, and noise. In practice, instead of using trial and error, it would be best if the system can dynamically adjust the duration of the batch interval based on inputs. Dynamic batch sizes in addition will have the ability to adjust to sudden changes in data incoming rates.

\paragraph{Input Serialization}
In Section 3, we evaluated the system using plain text input records. However, in real world applications, applications may choose to serialize their input data for better network latency. Serializing input data means they have to be deserialized during computation, so this decision concerns the trade-off between CPU and network I/O.

In the case of Spark Streaming, if the input data is serialized, it has the option to be directly stored inside the system as a block, bypassing the block interval and logic to convert stored records into blocks. Not surprisingly, this approach increases the rate at which Spark Streaming can intake data significantly. For the same benchmark with 20-byte records, Spark Streaming can sustain a throughput of 80MB/s, or 16M records per second using a single receiver.

The disadvantage of sending data directly as blocks is that they need to be coalesced at the application, i.e. the application needs to group input records together and send them as a block to Spark Streaming. This number should be relatively large: when Spark Stream receives blocks only containing single records, its throughput dropped to around 375KB/s, or 75K records/s.

%In this section we show that Spark Streaming is able to provide low end-to-end batch response times for a realistic and demanding workload. We also show that our contributions decrease the average batch latency by Y times and increases the scalability of the system from X tasks/s to Y tasks/s.

%In order to evaluate our contributions we have setup an instance of Spark Streaming to process a stream of tweets from Twitter. In our setup Spark Streaming runs in a dedicated cluster of 16 nodes. Each node has 16 cores  Intel IvyBridge 3.0GHz with 64GB of RAM. The nodes are interconnected with InfiniBand connections. We ran this experiment with 1 driver, 15 workers and X receivers.

%\paragraph {\bf Twitter Workload} To approximate as much as possible a real workload we have built a custom twitter receiver. This receiver listens for tweets using the standard Twitter API. Because this API samples the tweets (roughly 1\% of the tweets are published), every time we receive tweets from this 

%\paragraph{Producers, Consumers, and Queues}
%As we have found through our initial benchmark in Section 3, when the batch interval becomes shorter than the optimal, the end-to-end latency increases dramatically. From the average time break down shown in Figure~\ref{fig:Batchsize_vs_latency}, we can see a significant portion of the latency is spent receiving the data as well as sending the generated block to the Driver. What these two components share in common is that the data and metadata being moved around follow the producer/consumer pattern, with a queue acting as the middleman. As either side becomes overwhelmed, the size of the queue becomes larger, and latency increases. This architecture stems from SEDA~\cite{SEDA}, which was proposed as a way to serve web requests in multiple event queues. An interesting future work in this area would be to apply similar optimizations as have been done to SEDA, and compare the results.

\paragraph{Others} There are also a number of areas which we did not explore, either due to time or resource constraints. First, likely due to the reason that we only had access to 16 machines, scheduling and network communication were never bottlenecks. Second, as we are targeting very low latency streaming workloads, we did not benchmark in detail the performance of the system running applications that involve shuffles and aggregations, i.e. multiple stages per batch. Finally, we did not look into the implications of low latency in the Spark Streaming architecture when fault tolerance becomes a concern.
